{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect metrics\n",
    "For each dataset:\n",
    "1. Get similarity scores between chunks and question. We collect BERT scores, Jaccard similarities and BM25 scores.\n",
    "2. Apply the re-rankers to the chunks and record the ranks of the chunks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the current working directory to repo root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lovhag/Documents/Projects/rerankers-and-lexical-similarities'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../.\")\n",
    "os.getcwd() # check that we are in the right directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lovhag/Documents/Projects/rerankers-and-lexical-similarities/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_data = load_dataset(\"Lo/rerankers-and-lexical-similarities\", \"NQ\", split=\"standard\").to_pandas()\n",
    "litqao_data = load_dataset(\"Lo/rerankers-and-lexical-similarities\", \"LitQA2-o\", split=\"standard\").to_pandas()\n",
    "druid_data = load_dataset(\"Lo/rerankers-and-lexical-similarities\", \"DRUID\", split=\"standard\").to_pandas()\n",
    "druidq_data = load_dataset(\"Lo/rerankers-and-lexical-similarities\", \"DRUID\", split=\"prompt\").to_pandas()\n",
    "\n",
    "# chunks with prepended titles\n",
    "nq_t_data = load_dataset(\"Lo/rerankers-and-lexical-similarities\", \"NQ\", split=\"title\").to_pandas()\n",
    "litqao_t_data = load_dataset(\"Lo/rerankers-and-lexical-similarities\", \"LitQA2-o\", split=\"title\").to_pandas()\n",
    "druid_t_data = load_dataset(\"Lo/rerankers-and-lexical-similarities\", \"DRUID\", split=\"title\").to_pandas()\n",
    "\n",
    "# chunks with prepended contexts\n",
    "nq_c_data = load_dataset(\"Lo/rerankers-and-lexical-similarities\", \"NQ\", split=\"context\").to_pandas()\n",
    "litqao_c_data = load_dataset(\"Lo/rerankers-and-lexical-similarities\", \"LitQA2-o\", split=\"context\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get similarity scores\n",
    "\n",
    "Measure chunk similarity based on BERT score and Jaccard similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get BERT scores using a GPU\n",
    "\n",
    "To get the BERT scores, run the following script:\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bert_scores \\\n",
    "        --data_file <data-file> \n",
    "```\n",
    "\n",
    "`<data-file>` should be replaced by the path to the dataset with passages/chunks for which you would like to collect BERT scores, e.g. `data/DRUID/chunks.jsonl`. For this, you will want to use a GPU. We applied it to all of the data files loaded above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load BERT scores data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_data = nq_data.merge(pd.read_json(\"data/NQ/chunks_bert_scores.jsonl\", lines=True)['bert_score'], left_index=True, right_index=True)\n",
    "litqao_data = litqao_data.merge(pd.read_json(\"data/LitQA2-options/chunks_bert_scores.jsonl\", lines=True)['bert_score'], left_index=True, right_index=True)\n",
    "druid_data = druid_data.merge(pd.read_json(\"data/DRUID/chunks_bert_scores.jsonl\", lines=True)['bert_score'], left_index=True, right_index=True)\n",
    "druidq_data = druidq_data.merge(pd.read_json(\"data/DRUID-q/chunks_bert_scores.jsonl\", lines=True)['bert_score'], left_index=True, right_index=True)\n",
    "\n",
    "nq_t_data = nq_t_data.merge(pd.read_json(\"data/NQ/chunks_w_titles_bert_scores.jsonl\", lines=True)['bert_score'], left_index=True, right_index=True)\n",
    "litqao_t_data = litqao_t_data.merge(pd.read_json(\"data/LitQA2-options/chunks_w_titles_bert_scores.jsonl\", lines=True)['bert_score'], left_index=True, right_index=True)\n",
    "druid_t_data = druid_t_data.merge(pd.read_json(\"data/DRUID/chunks_w_titles_bert_scores.jsonl\", lines=True)['bert_score'], left_index=True, right_index=True)\n",
    "\n",
    "nq_c_data = nq_c_data.merge(pd.read_json(\"data/NQ/chunks_w_contexts_bert_scores.jsonl\", lines=True)['bert_score'], left_index=True, right_index=True)\n",
    "litqao_c_data = litqao_c_data.merge(pd.read_json(\"data/LitQA2-options/chunks_w_contexts_bert_scores.jsonl\", lines=True)['bert_score'], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Jaccard similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "SKIPLIST = set(list(punctuation) + [\"”\", \"“\", \"—\", \"’\", \"``\", \"''\"])\n",
    "\n",
    "def get_jaccard_sim(row):\n",
    "    def get_jaccard_index(s1, s2):\n",
    "        words_1 = set([w.lower() for w in word_tokenize(s1) if w.lower() not in SKIPLIST])\n",
    "        words_2 = set([w.lower() for w in word_tokenize(s2) if w.lower() not in SKIPLIST])\n",
    "        return 1-jaccard_distance(words_1, words_2)\n",
    "\n",
    "    if isinstance(row.chunks[0], dict):\n",
    "        return list(map(get_jaccard_index, [row.question]*len(row.chunks), [val[\"chunk\"] for val in row.chunks]))\n",
    "    else:\n",
    "        return list(map(get_jaccard_index, [row.question]*len(row.chunks), row.chunks))\n",
    "\n",
    "nq_data[\"jaccard_sim\"] = nq_data.apply(get_jaccard_sim, axis=1)\n",
    "litqao_data[\"jaccard_sim\"] = litqao_data.apply(get_jaccard_sim, axis=1)\n",
    "druid_data[\"jaccard_sim\"] = druid_data.apply(get_jaccard_sim, axis=1)\n",
    "druidq_data[\"jaccard_sim\"] = druidq_data.apply(get_jaccard_sim, axis=1)\n",
    "\n",
    "nq_t_data[\"jaccard_sim\"] = nq_t_data.apply(get_jaccard_sim, axis=1)\n",
    "litqao_t_data[\"jaccard_sim\"] = litqao_t_data.apply(get_jaccard_sim, axis=1)\n",
    "druid_t_data[\"jaccard_sim\"] = druid_t_data.apply(get_jaccard_sim, axis=1)\n",
    "\n",
    "nq_c_data[\"jaccard_sim\"] = nq_c_data.apply(get_jaccard_sim, axis=1)\n",
    "litqao_c_data[\"jaccard_sim\"] = litqao_c_data.apply(get_jaccard_sim, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get BM25 scores\n",
    "\n",
    "The following code will save the BM25 scores for all datasets to files titled `chunks_bm25_scores.jsonl`.\n",
    "\n",
    "**NQ**\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/NQ/chunks.jsonl\" \n",
    "```\n",
    "\n",
    "**LitQA2-options**\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/LitQA2-options/chunks.jsonl\" \n",
    "```\n",
    "\n",
    "**DRUID**\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/DRUID/chunks.jsonl\" \n",
    "```\n",
    "\n",
    "**DRUID-q**\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/DRUID-q/chunks.jsonl\" \n",
    "```\n",
    "\n",
    "**With titles**\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/NQ/chunks_w_titles.jsonl\" \n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/LitQA2-options/chunks_w_titles.jsonl\" \n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/DRUID/chunks_w_titles.jsonl\" \n",
    "```\n",
    "\n",
    "**With contexts**\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/NQ/chunks_w_contexts.jsonl\" \n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/LitQA2-options/chunks_w_contexts.jsonl\" \n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bm25_scores \\\n",
    "        --data_file \"data/DRUID/chunks_w_contexts.jsonl\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_data = nq_data.merge(pd.read_json(\"data/NQ/chunks_bm25_scores.jsonl\", lines=True)['bm25_score'], left_index=True, right_index=True)\n",
    "litqao_data = litqao_data.merge(pd.read_json(\"data/LitQA2-options/chunks_bm25_scores.jsonl\", lines=True)['bm25_score'], left_index=True, right_index=True)\n",
    "druid_data = druid_data.merge(pd.read_json(\"data/DRUID/chunks_bm25_scores.jsonl\", lines=True)['bm25_score'], left_index=True, right_index=True)\n",
    "druidq_data = druidq_data.merge(pd.read_json(\"data/DRUID-q/chunks_bm25_scores.jsonl\", lines=True)['bm25_score'], left_index=True, right_index=True)\n",
    "\n",
    "nq_t_data = nq_t_data.merge(pd.read_json(\"data/NQ/chunks_w_titles_bm25_scores.jsonl\", lines=True)['bm25_score'], left_index=True, right_index=True)\n",
    "litqao_t_data = litqao_t_data.merge(pd.read_json(\"data/LitQA2-options/chunks_w_titles_bm25_scores.jsonl\", lines=True)['bm25_score'], left_index=True, right_index=True)\n",
    "druid_t_data = druid_t_data.merge(pd.read_json(\"data/DRUID/chunks_w_titles_bm25_scores.jsonl\", lines=True)['bm25_score'], left_index=True, right_index=True)\n",
    "\n",
    "nq_c_data = nq_c_data.merge(pd.read_json(\"data/NQ/chunks_w_contexts_bm25_scores.jsonl\", lines=True)['bm25_score'], left_index=True, right_index=True)\n",
    "litqao_c_data = litqao_c_data.merge(pd.read_json(\"data/LitQA2-options/chunks_w_contexts_bm25_scores.jsonl\", lines=True)['bm25_score'], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply the rerankers to get reranker scores\n",
    "\n",
    "### Cohere reranker\n",
    "\n",
    "For this, you need to set up an Cohere API key and put it under `API-keys/cohere-api-key.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import time\n",
    "\n",
    "# Load Cohere API key\n",
    "with open(\"API-keys/cohere-api-key.txt\", \"r\") as f_co:\n",
    "    api_key = f_co.readline().strip()\n",
    "co = cohere.Client(api_key=api_key)\n",
    "\n",
    "RERANK_MAX_NBR_DOCUMENTS = 10000 # fixed by Cohere\n",
    "\n",
    "def process_sentences_for_rerank(sentences):\n",
    "    # truncate too long contexts\n",
    "    if len(sentences) > RERANK_MAX_NBR_DOCUMENTS:\n",
    "        sentences = sentences[:RERANK_MAX_NBR_DOCUMENTS]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3759/3759 [23:19<00:00,  2.69it/s]  \n",
      "100%|██████████| 124/124 [01:07<00:00,  1.84it/s]\n",
      "100%|██████████| 875/875 [04:42<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_cohere_reranker_scores(hs_data):\n",
    "    top_ixs = []\n",
    "    top_scores = []\n",
    "    for ix, row in tqdm(hs_data.iterrows(), total=len(hs_data)):\n",
    "        if isinstance(row.chunks[0], dict):\n",
    "            docs = [val[\"chunk\"] for val in row.chunks]\n",
    "        else:\n",
    "            docs = row.chunks\n",
    "        response = co.rerank(\n",
    "                model=\"rerank-english-v3.0\",\n",
    "                query=row.question,\n",
    "                documents=docs\n",
    "            )\n",
    "        tmp_top_ixs = [res.index for res in response.results]\n",
    "        top_ixs.append(tmp_top_ixs)\n",
    "        \n",
    "        tmp_top_scores = [res.relevance_score for res in response.results]\n",
    "        top_scores.append(tmp_top_scores)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    hs_data[\"reranker_top_ixs\"] = top_ixs\n",
    "    hs_data[\"reranker_top_scores\"] = top_scores\n",
    "    return hs_data\n",
    "\n",
    "nq_data = get_cohere_reranker_scores(nq_data)\n",
    "litqao_data = get_cohere_reranker_scores(litqao_data)\n",
    "druid_data = get_cohere_reranker_scores(druid_data)\n",
    "druidq_data = get_cohere_reranker_scores(druidq_data)\n",
    "\n",
    "nq_t_data = get_cohere_reranker_scores(nq_t_data)\n",
    "litqao_t_data = get_cohere_reranker_scores(litqao_t_data)\n",
    "druid_t_data = get_cohere_reranker_scores(druid_t_data)\n",
    "\n",
    "nq_c_data = get_cohere_reranker_scores(nq_c_data)\n",
    "litqao_c_data = get_cohere_reranker_scores(litqao_c_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_data.to_json(\"data/NQ/chunks_bert_reranker_scores.jsonl\", orient='records', lines=True)\n",
    "litqao_data.to_json(\"data/LitQA2-options/chunks_bert_reranker_scores.jsonl\", orient='records', lines=True)\n",
    "druid_data.to_json(\"data/DRUID/chunks_bert_reranker_scores.jsonl\", orient='records', lines=True)\n",
    "druidq_data.to_json(\"data/DRUID-q/chunks_bert_reranker_scores.jsonl\", orient='records', lines=True)\n",
    "\n",
    "nq_t_data.to_json(\"data/NQ/chunks_w_titles_bert_reranker_scores.jsonl\", orient='records', lines=True)\n",
    "litqao_t_data.to_json(\"data/LitQA2-options/chunks_w_titles_bert_reranker_scores.jsonl\", orient='records', lines=True)\n",
    "druid_t_data.to_json(\"data/DRUID/chunks_w_titles_bert_reranker_scores.jsonl\", orient='records', lines=True)\n",
    "\n",
    "nq_c_data.to_json(\"data/NQ/chunks_w_contexts_bert_reranker_scores.jsonl\", orient='records', lines=True)\n",
    "litqao_c_data.to_json(\"data/LitQA2-options/chunks_w_contexts_bert_reranker_scores.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAAI/bge-reranker-v2-gemma\n",
    "\n",
    "Run the following script to get the re-ranker scores:\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_bge_reranker_v2_gemma_ranks \\\n",
    "        --data_file <data-path> \n",
    "```\n",
    "\n",
    "Edit `<data-path>` as applicable to collect the re-ranker scores for all datasets covered above, e.g. `data/DRUID/chunks.jsonl`. For this, you will want to use a GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jinaai/jina-reranker-v1-turbo-en\n",
    "\n",
    "Run the following script to get the re-ranker scores:\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_jina_reranker_v1_turbo_en_ranks \\\n",
    "        --data_file <data-path> \n",
    "```\n",
    "\n",
    "Edit `<data-path>` as applicable to collect the re-ranker scores for all datasets covered above, e.g. `data/DRUID/chunks.jsonl`. For this, you will want to use a GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jinaai/jina-reranker-v2-base-multilingual\n",
    "\n",
    "Run the following script to get the re-ranker scores:\n",
    "\n",
    "```bash\n",
    "python -m src.collect_metrics.get_jina_reranker_v2_base_multilingual_ranks \\\n",
    "        --data_file <data-path> \n",
    "```\n",
    "\n",
    "Edit `<data-path>` as applicable to collect the re-ranker scores for all datasets covered above, e.g. `data/DRUID/chunks.jsonl`. For this, you will want to use a GPU. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
